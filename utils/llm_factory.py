"""
LLM Factory - LLM ìƒì„± ë¡œì§ ì¤‘ì•™í™”

ëª¨ë“  LLM ì¸ìŠ¤í„´ìŠ¤ëŠ” ì´ factoryë¥¼ í†µí•´ ìƒì„±ë©ë‹ˆë‹¤.
- OpenAI API
- vLLM (OpenAI compatible API)
- Ollama

ì‚¬ìš© ì˜ˆ:
    from utils.llm_factory import create_llm

    llm = create_llm(model_name="gpt-4.1-mini", temperature=0.7)
"""

from typing import Optional
from langchain.chat_models import init_chat_model
from config.settings import llm_config


def create_llm(
    model_name: Optional[str] = None,
    temperature: Optional[float] = None,
):
    """
    ì¤‘ì•™í™”ëœ LLM ìƒì„± í•¨ìˆ˜

    ì„¤ì • íŒŒì¼(config/settings.py)ì˜ LLM_PROVIDER ê°’ì— ë”°ë¼ ì ì ˆí•œ LLMì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        model_name: ëª¨ë¸ ì´ë¦„ (Noneì´ë©´ ì„¤ì • íŒŒì¼ì˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
        temperature: Temperature ì„¤ì • (Noneì´ë©´ ì„¤ì • íŒŒì¼ì˜ ê¸°ë³¸ê°’ ì‚¬ìš©)

    Returns:
        LangChain ChatModel ì¸ìŠ¤í„´ìŠ¤

    Examples:
        # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
        llm = create_llm()

        # ì»¤ìŠ¤í…€ ëª¨ë¸/temperature
        llm = create_llm(model_name="gpt-4o", temperature=0.9)

        # vLLM ì‚¬ìš© (.envì—ì„œ LLM_PROVIDER=vllm ì„¤ì •)
        llm = create_llm(model_name="meta-llama/Llama-3-8b-chat-hf")
    """
    # ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°
    model = model_name or llm_config.llm_model_name
    temp = temperature if temperature is not None else llm_config.llm_temperature

    provider = llm_config.llm_provider

    print(f"[ğŸ¤–] Creating LLM: provider={provider}, model={model}, temperature={temp}")

    if provider == "openai":
        return init_chat_model(
            model=model,
            model_provider="openai",
            temperature=temp,
        )

    elif provider == "vllm":
        # vLLMì€ OpenAI compatible APIë¥¼ ì œê³µ
        # base_urlì„ ì§ì ‘ ì „ë‹¬í•´ì•¼ í•¨
        base_url = llm_config.vllm_base_url

        # URLì— http:// ë˜ëŠ” https:// ì—†ìœ¼ë©´ ì¶”ê°€
        if not base_url.startswith(("http://", "https://")):
            base_url = f"http://{base_url}"

        print(f"[ğŸ”§] vLLM base_url: {base_url}")

        return init_chat_model(
            model=model,
            model_provider="openai",  # OpenAI compatible
            base_url=base_url,
            api_key=llm_config.vllm_api_key,
            temperature=temp,
        )

    elif provider == "ollama":
        return init_chat_model(
            model=model,
            model_provider="ollama",
            base_url=llm_config.ollama_base_url,
            temperature=temp,
        )

    else:
        raise ValueError(
            f"Unsupported LLM provider: {provider}. "
            f"Supported providers: 'openai', 'vllm', 'ollama'"
        )

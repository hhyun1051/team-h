"""
FastAPI Backend for Team-H Graph

í•µì‹¬ ê°œì„ :
1. TeamHGraph ì¸ìŠ¤í„´ìŠ¤ëŠ” ì•± ì‹œìž‘ ì‹œ í•œ ë²ˆë§Œ ìƒì„± (lifespan)
2. PostgreSQL checkpointerë¥¼ í†µí•´ thread_id ê¸°ë°˜ìœ¼ë¡œ ìƒíƒœ ìžë™ ë³µì›
3. ê° ìš”ì²­ì€ thread_idë§Œ ì „ë‹¬í•˜ì—¬ ê¸°ì¡´ ëŒ€í™” ìž¬ê°œ
4. astream_events()ë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ í† í° ìŠ¤íŠ¸ë¦¬ë°
5. Human-in-the-Loop ì¸í„°ëŸ½íŠ¸ ì²˜ë¦¬
"""

import sys
from pathlib import Path
from typing import Optional, Dict, Any
import os
import json
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv

# Project root setup
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

# Load .env
load_dotenv(project_root / ".env")

# Import agents
from agents.graph import TeamHGraph
from langgraph.types import Command

# Import Langfuse
from langfuse import Langfuse, get_client
from langfuse.langchain import CallbackHandler

# Import models
try:
    from .models import ChatRequest, ResumeRequest, InterruptResponse, StateResponse
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ ì‹œ (python main.py)
    from models import ChatRequest, ResumeRequest, InterruptResponse, StateResponse


# ============================================================================
# Global Agent Instance (initialized once at startup)
# ============================================================================

_agent: Optional[TeamHGraph] = None


def get_agent() -> TeamHGraph:
    """ì „ì—­ agent ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    if _agent is None:
        raise RuntimeError("Agent not initialized. This should not happen.")
    return _agent


# ============================================================================
# Application Lifespan
# ============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    FastAPI ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬

    ì•± ì‹œìž‘ ì‹œ TeamHGraphë¥¼ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³ ,
    ëª¨ë“  ìš”ì²­ì—ì„œ ìž¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    global _agent

    # Startup: Langfuse singleton ì´ˆê¸°í™” (middlewareê°€ ì‚¬ìš©)
    print("[ðŸ”§] Initializing Langfuse singleton...")
    Langfuse(
        public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
        secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
        host=os.getenv("LANGFUSE_BASE_URL", "http://localhost:3000"),
    )
    print("[âœ…] Langfuse singleton initialized")

    # Startup: Agent í•œ ë²ˆë§Œ ìƒì„±
    print("[ðŸš€] Initializing TeamHGraph (once)...")

    _agent = TeamHGraph(
        # Manager í™œì„±í™” (í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜)
        enable_manager_i=bool(os.getenv("HOMEASSISTANT_TOKEN")),
        enable_manager_m=True,
        enable_manager_s=bool(os.getenv("TAVILY_API_KEY")),
        enable_manager_t=bool(os.getenv("GOOGLE_CALENDAR_CREDENTIALS_PATH")),

        # Manager I ì„¤ì •
        homeassistant_url=os.getenv("HOMEASSISTANT_URL", "http://localhost:8124"),
        homeassistant_token=os.getenv("HOMEASSISTANT_TOKEN"),

        # Manager M ì„¤ì •
        embedding_type=os.getenv("EMBEDDING_TYPE", "openai"),
        embedder_url=os.getenv("EMBEDDER_URL"),
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        embedding_dims=int(os.getenv("OPENAI_EMBEDDING_DIMS", "3072")) if os.getenv("EMBEDDING_TYPE") == "openai" else int(os.getenv("FASTAPI_EMBEDDING_DIMS", "1024")),
        qdrant_url=os.getenv("QDRANT_URL", "http://localhost:6333"),
        qdrant_api_key=os.getenv("QDRANT_PASSWORD"),
        m_collection_name=os.getenv("MANAGER_M_COLLECTION", "manager_m_memories"),

        # Manager S ì„¤ì •
        tavily_api_key=os.getenv("TAVILY_API_KEY"),

        # Manager T ì„¤ì •
        google_credentials_path=os.getenv("GOOGLE_CALENDAR_CREDENTIALS_PATH"),
        google_token_path=os.getenv("GOOGLE_CALENDAR_TOKEN_PATH"),

        # LLM ì„¤ì • (.envì˜ LLM_MODEL_NAME, LLM_TEMPERATURE ì‚¬ìš©)
        model_name=os.getenv("LLM_MODEL_NAME", "gpt-4o-mini"),
        temperature=float(os.getenv("LLM_TEMPERATURE", "0.7")),

        # PostgreSQL checkpoint (ìžë™ ìƒíƒœ ì €ìž¥/ë³µì›)
        postgres_connection_string=os.getenv("POSTGRES_CONNECTION_STRING"),
        use_postgres_checkpoint=True,
    )

    # AsyncPostgresSaverì˜ í…Œì´ë¸” ì´ˆê¸°í™” (ë¹„ë™ê¸°)
    if hasattr(_agent.checkpointer, 'setup'):
        print("[ðŸ”§] Setting up PostgreSQL checkpoint tables...")
        await _agent.checkpointer.setup()
        print("[âœ…] PostgreSQL checkpoint tables ready")

    print("[âœ…] TeamHGraph initialized successfully")

    yield

    # Shutdown
    print("[ðŸ‘‹] FastAPI server shutting down...")


app = FastAPI(
    title="Team-H Graph API",
    description="LangGraph-based multi-agent system with streaming and HITL",
    version="2.0.0",
    lifespan=lifespan,
)

# CORS ì„¤ì • (Streamlitì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • originìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================================
# SSE Streaming Helper
# ============================================================================

async def generate_sse_stream(
    agent: TeamHGraph,
    config: Dict[str, Any],
    input_data: Any,
    context: Any = None,  # TeamHContext ì „ë‹¬
):
    """
    SSE (Server-Sent Events) ìŠ¤íŠ¸ë¦¼ ìƒì„±

    LangGraphì˜ astream_events()ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°:
    - on_chat_model_start: LLM í˜¸ì¶œ ì‹œìž‘
    - on_chat_model_stream: LLM í† í° ìŠ¤íŠ¸ë¦¬ë° (ì‹¤ì‹œê°„)
    - on_chat_model_end: LLM í˜¸ì¶œ ì™„ë£Œ
    - on_tool_start/end: íˆ´ ì‹¤í–‰ ìƒíƒœ
    - on_chain_start/end: ë…¸ë“œ ì‹¤í–‰ ìƒíƒœ
    - interrupt: HITL ì¸í„°ëŸ½íŠ¸

    ì°¸ê³ : docs/langchain_models.md, docs/langgraph_streaming.md

    Args:
        agent: TeamHGraph ì¸ìŠ¤í„´ìŠ¤ (ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìž¬ì‚¬ìš©)
        config: LangGraph config (thread_id í¬í•¨)
        input_data: ì´ˆê¸° ìž…ë ¥ ë˜ëŠ” Command
    """
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œìž‘ ì „ì— í˜„ìž¬ ìƒíƒœ ì¡°íšŒ (ì–´ë–¤ managerê°€ í™œì„±í™”ë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸)
        initial_snapshot = await agent.graph.aget_state(config)
        current_manager = initial_snapshot.values.get("current_agent") or initial_snapshot.values.get("last_active_manager")

        # ì´ˆê¸° manager ì •ë³´ ì „ì†¡
        if current_manager:
            init_data = {
                "event": "agent_start",
                "current_agent": current_manager,
            }
            yield f"data: {json.dumps(init_data, ensure_ascii=False)}\n\n"

        # astream_eventsë¡œ ëª¨ë“  ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
        # context ì „ë‹¬: toolsì˜ runtime.contextë¡œ ì ‘ê·¼ ê°€ëŠ¥
        async for event in agent.graph.astream_events(
            input_data,
            config,
            version="v2",  # v2ëŠ” ë” ìƒì„¸í•œ ì´ë²¤íŠ¸ ì œê³µ
            context=context,  # TeamHContext ì „ë‹¬
        ):
            event_type = event.get("event")
            event_name = event.get("name", "")
            event_data = event.get("data", {})
            metadata = event.get("metadata", {})

            # SSE í˜•ì‹ìœ¼ë¡œ ì „ì†¡í•  ë°ì´í„°
            sse_data = {
                "type": event_type,
                "name": event_name,
            }

            # ===== LLM í˜¸ì¶œ ì‹œìž‘ =====
            if event_type == "on_chat_model_start":
                sse_data["event"] = "llm_start"
                sse_data["node"] = metadata.get("langgraph_node", "unknown")
                yield f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"

            # ===== LLM í† í° ìŠ¤íŠ¸ë¦¬ë° (ì‹¤ì‹œê°„) =====
            elif event_type == "on_chat_model_stream":
                chunk = event_data.get("chunk", {})
                if not (hasattr(chunk, "content") and chunk.content):
                    continue  # ë‚´ìš© ì—†ëŠ” ì²­í¬ëŠ” ë¬´ì‹œ

                # ë¼ìš°í„° ë…¸ë“œì˜ LLM ìŠ¤íŠ¸ë¦¬ë°ì€ í•„í„°ë§ (router_decision ì´ë²¤íŠ¸ë¡œ ëŒ€ì²´)
                langgraph_node = metadata.get("langgraph_node", "")

                # ë””ë²„ê·¸: ë¼ìš°í„° ê´€ë ¨ ì´ë²¤íŠ¸ë§Œ ë¡œê·¸
                if "router" in langgraph_node.lower() or "router" in event_name.lower():
                    print(f"[DEBUG] Router stream - langgraph_node: '{langgraph_node}', event_name: '{event_name}', content: {chunk.content[:50] if chunk.content else 'None'}")

                if langgraph_node == "router":
                    print(f"[DEBUG] Router token FILTERED (not sent to client)")
                    continue  # ë¼ìš°í„° ë…¸ë“œì˜ í† í°ì€ ë¬´ì‹œ

                # Manager ë…¸ë“œì˜ í† í°ë§Œ ì „ì†¡
                print(f"[DEBUG] Sending token to client - node: '{langgraph_node}', content: {chunk.content[:30]}")
                sse_data["event"] = "token"
                sse_data["content"] = chunk.content
                # current_manager ì •ë³´ í¬í•¨ (Streamlitì—ì„œ agent ì•„ì´ì½˜ í‘œì‹œìš©)
                sse_data["current_agent"] = current_manager
                yield f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"

            # ===== LLM í˜¸ì¶œ ì™„ë£Œ =====
            elif event_type == "on_chat_model_end":
                langgraph_node = metadata.get("langgraph_node", "")

                # ë¼ìš°í„° ë…¸ë“œì˜ LLM ì™„ë£ŒëŠ” ë¬´ì‹œ (router_decision ì´ë²¤íŠ¸ë¡œ ëŒ€ì²´)
                if langgraph_node == "router":
                    print(f"[DEBUG] Router llm_end FILTERED (not sent to client)")
                    continue

                output = event_data.get("output", {})
                sse_data["event"] = "llm_end"
                if hasattr(output, "content"):
                    sse_data["full_message"] = output.content
                else:
                    sse_data["full_message"] = str(output)
                sse_data["node"] = langgraph_node
                yield f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"

            # ===== íˆ´ ì‹¤í–‰ ì‹œìž‘ =====
            elif event_type == "on_tool_start":
                sse_data["event"] = "tool_start"
                sse_data["tool_name"] = event_name
                sse_data["tool_input"] = event_data.get("input", {})
                sse_data["node"] = metadata.get("langgraph_node", "unknown")
                yield f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"

            # ===== íˆ´ ì‹¤í–‰ ì™„ë£Œ =====
            elif event_type == "on_tool_end":
                sse_data["event"] = "tool_end"
                sse_data["tool_name"] = event_name
                sse_data["tool_output"] = str(event_data.get("output"))
                sse_data["node"] = metadata.get("langgraph_node", "unknown")
                yield f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"

            # ===== ì²´ì¸/ë…¸ë“œ ì‹œìž‘ =====
            elif event_type == "on_chain_start":
                # Manager ë…¸ë“œ ì§„ìž… ê°ì§€ ë° current_manager ì—…ë°ì´íŠ¸
                langgraph_node = metadata.get("langgraph_node", "")
                if langgraph_node.startswith("manager_"):
                    # manager_m -> m ë³€í™˜
                    current_manager = langgraph_node.replace("manager_", "")
                    # Manager ë³€ê²½ ì•Œë¦¼
                    manager_change_data = {
                        "event": "agent_change",
                        "current_agent": current_manager,
                    }
                    yield f"data: {json.dumps(manager_change_data, ensure_ascii=False)}\n\n"

                # ê·¸ëž˜í”„ ë…¸ë“œ ì‹œìž‘ (router, manager_i, manager_m ë“±)
                sse_data["event"] = "node_start"
                sse_data["node_name"] = event_name
                yield f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"

            # ===== ì²´ì¸/ë…¸ë“œ ì™„ë£Œ =====
            elif event_type == "on_chain_end":
                langgraph_node = metadata.get("langgraph_node", "")
                output = event_data.get("output")

                # ë””ë²„ê·¸: ëª¨ë“  chain_end ì´ë²¤íŠ¸ ë¡œê¹…
                print(f"[DEBUG] on_chain_end - langgraph_node: '{langgraph_node}', event_name: '{event_name}', output_type: {type(output).__name__}, has_target_agent: {hasattr(output, 'target_agent') if output else False}")

                # ë¼ìš°í„° ë…¸ë“œ ì™„ë£Œ ì‹œ íŠ¹ë³„ ì²˜ë¦¬ (AgentRouting ê°ì²´ë§Œ)
                # RunnableSequenceë§Œ ì„ íƒ: RunnableLambda(ë‚´ë¶€), RunnableSequence(ì¤‘ê°„), router(ìµœì¢… Command) ì¤‘ RunnableSequenceì—ì„œë§Œ emit
                if (langgraph_node == "router" and event_name == "RunnableSequence" and output and
                    hasattr(output, "target_agent") and hasattr(output, "reason")):
                    print(f"[DEBUG] Sending router_decision: target={output.target_agent}, reason={output.reason[:50]}")
                    router_data = {
                        "event": "router_decision",
                        "target_agent": output.target_agent,
                        "reason": output.reason,
                    }
                    yield f"data: {json.dumps(router_data, ensure_ascii=False)}\n\n"
                    # ë¼ìš°í„° ê²°ì •ì€ node_end ì´ë²¤íŠ¸ë¥¼ ì „ì†¡í•˜ì§€ ì•ŠìŒ (ì¤‘ë³µ ë°©ì§€)
                    continue

                # ì¼ë°˜ ë…¸ë“œ ì™„ë£Œ ì´ë²¤íŠ¸
                sse_data["event"] = "node_end"
                sse_data["node_name"] = event_name
                if output:
                    # ë…¸ë“œ ê²°ê³¼ ìš”ì•½ë§Œ ì „ì†¡ (ë„ˆë¬´ í¬ë©´ ìƒëžµ)
                    output_str = str(output)
                    if len(output_str) > 500:
                        output_str = output_str[:500] + "..."
                    sse_data["output"] = output_str
                yield f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"

        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ìµœì¢… ìƒíƒœ í™•ì¸ (ì¸í„°ëŸ½íŠ¸ ì²´í¬)
        snapshot = await agent.graph.aget_state(config)

        # ìµœì¢… ìƒíƒœì—ì„œ current_agent ì •ë³´ ì¶”ì¶œ
        final_current_agent = snapshot.values.get("current_agent") or snapshot.values.get("last_active_manager")

        if snapshot.next:
            # ì¸í„°ëŸ½íŠ¸ ë°œìƒ
            interrupts = []
            for task in snapshot.tasks:
                interrupts.extend(task.interrupts)

            if interrupts:
                interrupt_data = {
                    "event": "interrupt",
                    "type": "interrupt",
                    "interrupt": interrupts[0].value,
                    "thread_id": config["configurable"]["thread_id"],
                }
                yield f"data: {json.dumps(interrupt_data, ensure_ascii=False, default=str)}\n\n"
        else:
            # ì •ìƒ ì™„ë£Œ
            final_data = {
                "event": "done",
                "type": "done",
                "messages_count": len(snapshot.values.get("messages", [])),
                "current_agent": snapshot.values.get("current_agent"),
                "handoff_count": snapshot.values.get("handoff_count", 0),
            }
            yield f"data: {json.dumps(final_data, ensure_ascii=False)}\n\n"

    except Exception as e:
        import traceback
        error_data = {
            "event": "error",
            "type": "error",
            "error": str(e),
            "traceback": traceback.format_exc(),
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"


# ============================================================================
# API Endpoints
# ============================================================================

@app.get("/")
async def root():
    """Health check"""
    return {
        "status": "ok",
        "service": "Team-H Graph API",
        "version": "2.0.0",
        "agent_initialized": _agent is not None,
    }


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸

    í•µì‹¬:
    - Agent ìž¬ì‚¬ìš© (ì „ì—­ ì¸ìŠ¤í„´ìŠ¤)
    - thread_idë¡œ ìƒíƒœ ìžë™ ë³µì› (PostgreSQL checkpointer)
    - astream_events()ë¡œ ì‹¤ì‹œê°„ í† í° ìŠ¤íŠ¸ë¦¬ë°
    - Langfuse CallbackHandlerë¡œ ì „ì²´ íë¦„ ë¡œê¹…

    Args:
        request: ChatRequest (message, thread_id, user_id, session_id)

    Returns:
        StreamingResponse: SSE ìŠ¤íŠ¸ë¦¼
    """
    try:
        agent = get_agent()  # ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìž¬ì‚¬ìš©

        session_id = request.session_id or request.thread_id

        # CallbackHandler ìƒì„±
        langfuse_handler = CallbackHandler()

        # Config (thread_id + callbacks + metadata)
        config = {
            "configurable": {
                "thread_id": request.thread_id,
            },
            "callbacks": [langfuse_handler],  # Langfuse ì „ì²´ íë¦„ ë¡œê¹…
            "metadata": {
                "langfuse_user_id": request.user_id,
                "langfuse_session_id": session_id,
                "langfuse_tags": ["team-h", "api", "streaming"],
            }
        }

        # Context ìƒì„± (TeamHContext - toolsì˜ runtime.contextë¡œ ì „ë‹¬)
        from agents.context import TeamHContext
        context = TeamHContext(
            user_id=request.user_id,
            thread_id=request.thread_id,
            session_id=session_id,
        )

        # ì´ˆê¸° ìž…ë ¥
        from langchain_core.messages import HumanMessage
        initial_state = {
            "messages": [HumanMessage(content=request.message)],
            "handoff_count": 0,
        }

        # SSE ìŠ¤íŠ¸ë¦¼ ë°˜í™˜
        return StreamingResponse(
            generate_sse_stream(agent, config, initial_state, context),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/resume")
async def chat_resume(request: ResumeRequest):
    """
    HITL ìž¬ê°œ ì—”ë“œí¬ì¸íŠ¸

    ì¸í„°ëŸ½íŠ¸ëœ ëŒ€í™”ë¥¼ ì‚¬ìš©ìž ê²°ì •(ìŠ¹ì¸/ê±°ë¶€/íŽ¸ì§‘)ì„ ê¸°ë°˜ìœ¼ë¡œ ìž¬ê°œí•©ë‹ˆë‹¤.

    í•µì‹¬:
    - ê°™ì€ agent ìž¬ì‚¬ìš© (ì „ì—­ ì¸ìŠ¤í„´ìŠ¤)
    - thread_idë¡œ ìƒíƒœ ë³µì›
    - Command(resume=...)ë¡œ ì¸í„°ëŸ½íŠ¸ ìž¬ê°œ
    - Langfuse CallbackHandlerë¡œ ì „ì²´ íë¦„ ë¡œê¹…

    Args:
        request: ResumeRequest (thread_id, decisions, user_id, session_id)

    Returns:
        StreamingResponse: SSE ìŠ¤íŠ¸ë¦¼
    """
    try:
        agent = get_agent()  # ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìž¬ì‚¬ìš©

        session_id = request.session_id or request.thread_id

        # CallbackHandler ìƒì„±
        langfuse_handler = CallbackHandler()

        # Config (thread_id + callbacks + metadata)
        config = {
            "configurable": {
                "thread_id": request.thread_id,
            },
            "callbacks": [langfuse_handler],  # Langfuse ì „ì²´ íë¦„ ë¡œê¹…
            "metadata": {
                "langfuse_user_id": request.user_id,
                "langfuse_session_id": session_id,
                "langfuse_tags": ["team-h", "api", "resume"],
            }
        }

        # Context ìƒì„± (TeamHContext - toolsì˜ runtime.contextë¡œ ì „ë‹¬)
        from agents.context import TeamHContext
        context = TeamHContext(
            user_id=request.user_id,
            thread_id=request.thread_id,
            session_id=session_id,
        )

        # Command ìƒì„±
        command = Command(resume={"decisions": request.decisions})

        # SSE ìŠ¤íŠ¸ë¦¼ ë°˜í™˜
        return StreamingResponse(
            generate_sse_stream(agent, config, command, context),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/state/{thread_id}")
async def get_state(thread_id: str):
    """
    íŠ¹ì • threadì˜ í˜„ìž¬ ìƒíƒœ ì¡°íšŒ

    Args:
        thread_id: ëŒ€í™” ìŠ¤ë ˆë“œ ID

    Returns:
        StateResponse: í˜„ìž¬ ê·¸ëž˜í”„ ìƒíƒœ, ë‹¤ìŒ ë…¸ë“œ, ì¸í„°ëŸ½íŠ¸ ì—¬ë¶€
    """
    try:
        agent = get_agent()
        config = {"configurable": {"thread_id": thread_id}}

        # ìƒíƒœ ì¡°íšŒ
        snapshot = await agent.graph.aget_state(config)

        # ì¸í„°ëŸ½íŠ¸ í™•ì¸
        interrupts = []
        if snapshot.next:
            for task in snapshot.tasks:
                interrupts.extend(task.interrupts)

        return {
            "status": "success",
            "thread_id": thread_id,
            "state": snapshot.values,
            "next_nodes": snapshot.next,
            "has_interrupt": len(interrupts) > 0,
            "interrupts": [interrupt.value for interrupt in interrupts],
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
    )
